{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_13303/1960103714.py:11: DeprecationWarning: Please import `zoom` from the `scipy.ndimage` namespace; the `scipy.ndimage.interpolation` namespace is deprecated and will be removed in SciPy 2.0.0.\n",
      "  from scipy.ndimage.interpolation import zoom\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "\n",
    "from scipy import ndimage\n",
    "from scipy.ndimage.interpolation import zoom\n",
    "\n",
    "# import albumentations as A\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.modules.loss import CrossEntropyLoss\n",
    "from torchvision import transforms\n",
    "from torchsummaryX import summary\n",
    "import torch.optim as optim\n",
    "from utils import DiceLoss\n",
    "import torch.backends.cudnn as cudnn\n",
    "from networks.vit_seg_modeling import VisionTransformer as ViT_seg\n",
    "from networks.vit_seg_modeling import CONFIGS as CONFIGS_ViT_seg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fb1bd571870>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# set seed\n",
    "seed = 304\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "GLOB_PATH = '/home/darpan/Desktop/7thSem/dlProject/data'\n",
    "\n",
    "# Path to the director where images for semantic segmentation are stored\n",
    "IMAGES_DIR = f'{GLOB_PATH}/Image/'\n",
    "# Path to the directory where labels for semantic segmentation are stored\n",
    "LABELS_DIR = f'{GLOB_PATH}/Mask/'\n",
    "IMG_EXT = 'jpg'\n",
    "LABEL_EXT = 'png'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Image</th>\n",
       "      <th>Mask</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.jpg</td>\n",
       "      <td>0.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.jpg</td>\n",
       "      <td>1.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.jpg</td>\n",
       "      <td>2.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.jpg</td>\n",
       "      <td>3.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.jpg</td>\n",
       "      <td>4.png</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Image   Mask\n",
       "0  0.jpg  0.png\n",
       "1  1.jpg  1.png\n",
       "2  2.jpg  2.png\n",
       "3  3.jpg  3.png\n",
       "4  4.jpg  4.png"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metadata = pd.read_csv(f'{GLOB_PATH}/metadata.csv')\n",
    "metadata.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_images_labels(IMAGES_DIR, LABELS_DIR, metadata):\n",
    "    \"\"\"\n",
    "    Return the list of all valid images and labels\n",
    "    \"\"\"\n",
    "    images = []\n",
    "    labels = []\n",
    "    min_h = 1e6; max_h = 0\n",
    "    min_w = 1e6; max_w = 0\n",
    "    for _, row in metadata.iterrows():\n",
    "        image = os.path.join(IMAGES_DIR, row['Image'])\n",
    "        label = os.path.join(LABELS_DIR, row['Mask'])\n",
    "        img_arr = np.array(Image.open(image))\n",
    "        label_arr = np.array(Image.open(label))\n",
    "        if (img_arr.ndim == 3) and (img_arr.shape[2] == 3) and (img_arr.shape[:-1] == label_arr.shape):\n",
    "            min_h = min(min_h, img_arr.shape[0]); max_h = max(max_h, img_arr.shape[0])\n",
    "            min_w = min(min_w, img_arr.shape[1]); max_w = max(max_w, img_arr.shape[1])\n",
    "            images.append(image)\n",
    "            labels.append(label)\n",
    "        else:\n",
    "            print(f\"Skipping image {row['Image']} and mask {row['Mask']}\")\n",
    "    print(f\"Min height: {min_h}, Max height: {max_h}\")\n",
    "    print(f\"Min width: {min_w}, Max width: {max_w}\")\n",
    "    return images, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping image 0.jpg and mask 0.png\n",
      "Skipping image 2.jpg and mask 2.png\n",
      "Skipping image 14.jpg and mask 14.png\n",
      "Skipping image 15.jpg and mask 15.png\n",
      "Skipping image 2052.jpg and mask 2052.png\n",
      "Skipping image 2053.jpg and mask 2053.png\n",
      "Skipping image 3048.jpg and mask 3048.png\n",
      "Skipping image 3059.jpg and mask 3059.png\n",
      "Skipping image 1061.jpg and mask 1061.png\n",
      "Skipping image 1079.jpg and mask 1079.png\n",
      "Min height: 219, Max height: 3648\n",
      "Min width: 330, Max width: 5472\n",
      "280 280\n",
      "['/home/darpan/Desktop/7thSem/dlProject/data/Image/1.jpg', '/home/darpan/Desktop/7thSem/dlProject/data/Image/3.jpg', '/home/darpan/Desktop/7thSem/dlProject/data/Image/4.jpg', '/home/darpan/Desktop/7thSem/dlProject/data/Image/5.jpg', '/home/darpan/Desktop/7thSem/dlProject/data/Image/6.jpg']\n",
      "['/home/darpan/Desktop/7thSem/dlProject/data/Mask/1.png', '/home/darpan/Desktop/7thSem/dlProject/data/Mask/3.png', '/home/darpan/Desktop/7thSem/dlProject/data/Mask/4.png', '/home/darpan/Desktop/7thSem/dlProject/data/Mask/5.png', '/home/darpan/Desktop/7thSem/dlProject/data/Mask/6.png']\n"
     ]
    }
   ],
   "source": [
    "# Take the first TRAIN_SIZE images for training\n",
    "all_images, all_labels = get_all_images_labels(IMAGES_DIR, LABELS_DIR, metadata)\n",
    "\n",
    "print(len(all_images), len(all_labels))\n",
    "print(all_images[:5])\n",
    "print(all_labels[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_SIZE = 200 # Number of images to use for training\n",
    "VAL_SIZE = 20 # Number of images to use for validation\n",
    "TEST_SIZE = 60 # Number of images to use for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/home/darpan/Desktop/7thSem/dlProject/data/Image/1.jpg', '/home/darpan/Desktop/7thSem/dlProject/data/Image/3.jpg', '/home/darpan/Desktop/7thSem/dlProject/data/Image/4.jpg', '/home/darpan/Desktop/7thSem/dlProject/data/Image/5.jpg', '/home/darpan/Desktop/7thSem/dlProject/data/Image/6.jpg']\n",
      "['/home/darpan/Desktop/7thSem/dlProject/data/Mask/1.png', '/home/darpan/Desktop/7thSem/dlProject/data/Mask/3.png', '/home/darpan/Desktop/7thSem/dlProject/data/Mask/4.png', '/home/darpan/Desktop/7thSem/dlProject/data/Mask/5.png', '/home/darpan/Desktop/7thSem/dlProject/data/Mask/6.png']\n",
      "['/home/darpan/Desktop/7thSem/dlProject/data/Image/1006.jpg', '/home/darpan/Desktop/7thSem/dlProject/data/Image/1007.jpg', '/home/darpan/Desktop/7thSem/dlProject/data/Image/1008.jpg', '/home/darpan/Desktop/7thSem/dlProject/data/Image/1009.jpg', '/home/darpan/Desktop/7thSem/dlProject/data/Image/1010.jpg']\n",
      "['/home/darpan/Desktop/7thSem/dlProject/data/Mask/1006.png', '/home/darpan/Desktop/7thSem/dlProject/data/Mask/1007.png', '/home/darpan/Desktop/7thSem/dlProject/data/Mask/1008.png', '/home/darpan/Desktop/7thSem/dlProject/data/Mask/1009.png', '/home/darpan/Desktop/7thSem/dlProject/data/Mask/1010.png']\n",
      "['/home/darpan/Desktop/7thSem/dlProject/data/Image/1026.jpg', '/home/darpan/Desktop/7thSem/dlProject/data/Image/1027.jpg', '/home/darpan/Desktop/7thSem/dlProject/data/Image/1028.jpg', '/home/darpan/Desktop/7thSem/dlProject/data/Image/1029.jpg', '/home/darpan/Desktop/7thSem/dlProject/data/Image/1030.jpg']\n",
      "['/home/darpan/Desktop/7thSem/dlProject/data/Mask/1026.png', '/home/darpan/Desktop/7thSem/dlProject/data/Mask/1027.png', '/home/darpan/Desktop/7thSem/dlProject/data/Mask/1028.png', '/home/darpan/Desktop/7thSem/dlProject/data/Mask/1029.png', '/home/darpan/Desktop/7thSem/dlProject/data/Mask/1030.png']\n"
     ]
    }
   ],
   "source": [
    "image_paths_train = all_images[:TRAIN_SIZE]\n",
    "label_paths_train = all_labels[:TRAIN_SIZE]\n",
    "\n",
    "print(image_paths_train[:5])\n",
    "print(label_paths_train[:5])\n",
    "\n",
    "image_paths_val = all_images[TRAIN_SIZE:TRAIN_SIZE+VAL_SIZE]\n",
    "label_paths_val = all_labels[TRAIN_SIZE:TRAIN_SIZE+VAL_SIZE]\n",
    "\n",
    "print(image_paths_val[:5])\n",
    "print(label_paths_val[:5])\n",
    "\n",
    "image_paths_test = all_images[-TEST_SIZE:]\n",
    "label_paths_test = all_labels[-TEST_SIZE:]\n",
    "\n",
    "print(image_paths_test[:5])\n",
    "print(label_paths_test[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** White color in mask means flooded area and black color means non-flooded area."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    # resize the image to (256, 256)\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.ToTensor(),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class floodDataset(Dataset):\n",
    "    def __init__(self, image_paths, label_paths, transform=None):\n",
    "        self.image_paths = image_paths\n",
    "        self.label_paths = label_paths\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image = Image.open(self.image_paths[idx])\n",
    "        label = Image.open(self.label_paths[idx])\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "            label = self.transform(label)\n",
    "        # convert label to binary\n",
    "        label = (label > 0).float()\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_train = floodDataset(image_paths_train, label_paths_train, transform=transform)\n",
    "db_test = floodDataset(image_paths_test, label_paths_test, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainloader = DataLoader(db_train, batch_size=BATCH_SIZE, shuffle=True)\n",
    "testloader = DataLoader(db_test, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# device = \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Download the R50+ViT-B_16.npz, model from `https://console.cloud.google.com/storage/browser/vit_models/imagenet21k`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load_pretrained: grid-size from 14 to 16\n"
     ]
    }
   ],
   "source": [
    "MODEL_NAME = 'R50-ViT-B_16'\n",
    "config_vit = CONFIGS_ViT_seg[MODEL_NAME]\n",
    "config_vit.n_classes = 2\n",
    "model = ViT_seg(config_vit, img_size=256, num_classes=config_vit.n_classes).to(device)\n",
    "model.load_from(weights=np.load('/home/darpan/Desktop/7thSem/dlProject/src/TransUnet/model/R50+ViT-B_16.npz'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train()\n",
    "LR = 0.01\n",
    "num_classes = 2\n",
    "ce_loss = CrossEntropyLoss()\n",
    "dice_loss = DiceLoss(num_classes)\n",
    "MOMENTUM = 0.9\n",
    "DECAY = 1e-4\n",
    "opt_name = 'sgd'\n",
    "optimizer = optim.SGD(model.parameters(), lr=LR, momentum=MOMENTUM, weight_decay=DECAY)\n",
    "MAX_EPOCHS = 100\n",
    "\n",
    "# iter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_dir = './results'\n",
    "exp_name = f'{MODEL_NAME}_lr_{LR}_epochs_{MAX_EPOCHS}_test'\n",
    "os.makedirs(f'{res_dir}/{exp_name}', exist_ok=True)\n",
    "\n",
    "config = {\n",
    "    'model': MODEL_NAME,\n",
    "    'lr': LR,\n",
    "    'epochs': MAX_EPOCHS,\n",
    "    'batch_size': BATCH_SIZE,\n",
    "    'optimizer': opt_name,\n",
    "    'momentum': MOMENTUM,\n",
    "    'decay': DECAY,\n",
    "    'seed': seed\n",
    "}\n",
    "\n",
    "with open(f'{res_dir}/{exp_name}/config.json', 'w') as f:\n",
    "    json.dump(config, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model(model, testloader, optimizer, log_file, work='test'):\n",
    "    model.eval()\n",
    "    iou_score = 0\n",
    "    dice_score = 0\n",
    "    loss_t = 0\n",
    "    loss_ce_t = 0\n",
    "    loss_dice_t = 0\n",
    "    for i, data in enumerate(testloader):\n",
    "        image, label = data\n",
    "\n",
    "        image, label = image.to(device), label.to(device)\n",
    "        outputs = model(image)\n",
    "\n",
    "        label = label.squeeze(1)\n",
    "        label2 = torch.nn.functional.one_hot(label.long(), num_classes=2).permute(0, 3, 1, 2).float()\n",
    "        # print(outputs.shape, label2.shape)\n",
    "        # print(outputs.long().dtype, label.dtype)\n",
    "        loss_ce = ce_loss(outputs, label2)\n",
    "        loss_dice = dice_loss(outputs, label)\n",
    "        loss = 0.5 * loss_ce + 0.5 * loss_dice\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # # calulate iou score and dice score\n",
    "        outputs = torch.argmax(outputs, dim=1)\n",
    "        label = label.squeeze(1)\n",
    "        \n",
    "\n",
    "        tp = torch.sum(outputs * label)\n",
    "        fp = torch.sum(outputs * (1 - label))\n",
    "        fn = torch.sum((1 - outputs) * label)\n",
    "        iou = tp / (tp + fp + fn)\n",
    "        dice = 2 * tp / (2 * tp + fp + fn)\n",
    "\n",
    "        \n",
    "        iou_score += iou\n",
    "        dice_score += dice\n",
    "        loss_t += loss\n",
    "        loss_ce_t += loss_ce\n",
    "        loss_dice_t += loss_dice\n",
    "        # break\n",
    "    iou_score /= len(testloader)\n",
    "    dice_score /= len(testloader)\n",
    "    loss_t /= len(testloader)\n",
    "    loss_ce_t /= len(testloader)\n",
    "    loss_dice_t /= len(testloader)\n",
    "    print(f'{work} Data: IoU: {iou_score}, Dice: {dice_score}, Loss: {loss_t}, CE Loss: {loss_ce_t}, Dice Loss: {loss_dice_t}')\n",
    "    log_file.write(f'{work} Data: IoU: {iou_score}, Dice: {dice_score}, Loss: {loss_t}, CE Loss: {loss_ce_t}, Dice Loss: {loss_dice_t}\\n')\n",
    "\n",
    "    return iou_score, dice_score, [loss_t, loss_ce_t, loss_dice_t]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, trainloader, testloader, valloader, optimizer, log_file, max_epoch=100):\n",
    "    \n",
    "    iterator = tqdm(range(max_epoch), ncols=70)\n",
    "    iter_num = 0\n",
    "    iou_val_best = 0\n",
    "    iou_test_best = 0\n",
    "    for epoch_num in iterator:\n",
    "        model.train()\n",
    "        iou_score = 0\n",
    "        dice_score = 0\n",
    "        loss_t = 0\n",
    "        loss_ce_t = 0\n",
    "        loss_dice_t = 0\n",
    "        for i, data in enumerate(trainloader):\n",
    "            image, label = data\n",
    "\n",
    "            image, label = image.to(device), label.to(device)\n",
    "            outputs = model(image)\n",
    "\n",
    "            label = label.squeeze(1)\n",
    "            label2 = torch.nn.functional.one_hot(label.long(), num_classes=2).permute(0, 3, 1, 2).float()\n",
    "            loss_ce = ce_loss(outputs, label2)\n",
    "            loss_dice = dice_loss(outputs, label)\n",
    "            loss = 0.5 * loss_ce + 0.5 * loss_dice\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            iter_num += 1\n",
    "\n",
    "            outputs = torch.argmax(outputs, dim=1)\n",
    "            label = label.squeeze(1)\n",
    "            tp = torch.sum(outputs * label)\n",
    "            fp = torch.sum(outputs * (1 - label))\n",
    "            fn = torch.sum((1 - outputs) * label)\n",
    "            iou = tp / (tp + fp + fn)\n",
    "            dice = 2 * tp / (2 * tp + fp + fn)\n",
    "\n",
    "            # print(f\"Epoch: {epoch_num}, Iteration: {i}, Loss: {loss}, CE Loss: {loss_ce}, Dice Loss: {loss_dice}, IoU: {iou}, Dice: {dice}\")\n",
    "            log_file.write(f\"Epoch: {epoch_num}, Iteration: {i}, Loss: {loss}, CE Loss: {loss_ce}, Dice Loss: {loss_dice}, IoU: {iou}, Dice: {dice}\\n\")\n",
    "            iou_score += iou\n",
    "            dice_score += dice\n",
    "            loss_t += loss\n",
    "            loss_ce_t += loss_ce\n",
    "            loss_dice_t += loss_dice\n",
    "        iou_score /= len(trainloader)\n",
    "        dice_score /= len(trainloader)\n",
    "        loss_t /= len(trainloader)\n",
    "        loss_ce_t /= len(trainloader)\n",
    "        loss_dice_t /= len(trainloader)\n",
    "        print(f\"Epoch: {epoch_num}, Loss: {loss_t}, CE Loss: {loss_ce_t}, Dice Loss: {loss_dice_t}, IoU: {iou_score}, Dice: {dice_score}\")\n",
    "        log_file.write(f\"Epoch: {epoch_num}, Loss: {loss_t}, CE Loss: {loss_ce_t}, Dice Loss: {loss_dice_t}, IoU: {iou_score}, Dice: {dice_score}\\n\")\n",
    "\n",
    "        iou_val, dice_val, loss_val = eval_model(model, valloader, optimizer, log_file, work='val')\n",
    "        iou_test, dice_test, loss_test = eval_model(model, testloader, optimizer, log_file, work='test')\n",
    "\n",
    "        if (iou_val > iou_val_best):\n",
    "            iou_val_best = iou_val\n",
    "            torch.save(model.state_dict(), f'{res_dir}/{exp_name}/best_model_val.pth')\n",
    "            log_file.write(f\"Best model val saved at epoch {epoch_num} with IoU: {iou_val_best}\\n\")\n",
    "\n",
    "        if (iou_test > iou_test_best):\n",
    "            iou_test_best = iou_test\n",
    "            torch.save(model.state_dict(), f'{res_dir}/{exp_name}/best_model_test.pth')\n",
    "            log_file.write(f\"Best model test saved at epoch {epoch_num} with IoU: {iou_test_best}\\n\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                         | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss: 0.42036959528923035, CE Loss: 0.5232509970664978, Dice Loss: 0.3174884021282196, IoU: 0.5550721883773804, Dice: 0.7072895169258118\n",
      "val Data: IoU: 0.5978606343269348, Dice: 0.7449324727058411, Loss: 0.3309345245361328, CE Loss: 0.4541394114494324, Dice Loss: 0.20772963762283325\n",
      "test Data: IoU: 0.6250632405281067, Dice: 0.7688474655151367, Loss: 0.3135635554790497, CE Loss: 0.4428594708442688, Dice Loss: 0.18426768481731415\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▎                              | 1/100 [00:40<1:06:30, 40.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Loss: 0.306387335062027, CE Loss: 0.4409915804862976, Dice Loss: 0.17178329825401306, IoU: 0.6837534308433533, Dice: 0.8096258640289307\n",
      "val Data: IoU: 0.7134944200515747, Dice: 0.8309220671653748, Loss: 0.25423356890678406, CE Loss: 0.3692939281463623, Dice Loss: 0.1391732096672058\n",
      "test Data: IoU: 0.7302210927009583, Dice: 0.8434285521507263, Loss: 0.24356575310230255, CE Loss: 0.3566155433654785, Dice Loss: 0.130515918135643\n"
     ]
    }
   ],
   "source": [
    "log_file = open(f'{res_dir}/{exp_name}/log.txt', 'w')\n",
    "\n",
    "train_model(model, trainloader, testloader, testloader, optimizer, log_file, max_epoch=MAX_EPOCHS)\n",
    "\n",
    "torch.save(model.state_dict(), f'{res_dir}/{exp_name}/final_model.pth')\n",
    "\n",
    "iou_test, dice_test, _ = eval_model(model, testloader, optimizer, log_file, work='test')\n",
    "\n",
    "log_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters: 105322146\n"
     ]
    }
   ],
   "source": [
    "# print number of parameters\n",
    "num_params = sum(p.numel() for p in model.parameters())\n",
    "print(f'Number of parameters: {num_params}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
