{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "12bbf730",
   "metadata": {},
   "source": [
    "# UNet++ for Flood Area Segmentation\n",
    "This notebook implements UNet++ on the flood area segmentation dataset. It leverages the `segmentation_models_pytorch` library for model implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bf56e334",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/procoder/Documents/Deep Learning/dlProject/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/procoder/Documents/Deep Learning/dlProject/.venv/lib/python3.11/site-packages/albumentations/check_version.py:51: UserWarning: Error fetching version info The read operation timed out\n",
      "  data = fetch_version_info()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3.5dev0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import albumentations as A\n",
    "import segmentation_models_pytorch as smp\n",
    "print(smp.__version__)\n",
    "\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b228d41",
   "metadata": {},
   "source": [
    "# Load Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fb999e5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Image</th>\n",
       "      <th>Mask</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.jpg</td>\n",
       "      <td>0.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.jpg</td>\n",
       "      <td>1.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.jpg</td>\n",
       "      <td>2.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.jpg</td>\n",
       "      <td>3.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.jpg</td>\n",
       "      <td>4.png</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Image   Mask\n",
       "0  0.jpg  0.png\n",
       "1  1.jpg  1.png\n",
       "2  2.jpg  2.png\n",
       "3  3.jpg  3.png\n",
       "4  4.jpg  4.png"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define paths to images and masks\n",
    "IMAGES_DIR = 'data/Image/'\n",
    "LABELS_DIR = 'data/Mask/'\n",
    "\n",
    "# Read metadata\n",
    "metadata = pd.read_csv('data/metadata.csv')\n",
    "metadata.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "858eeacf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping image 0.jpg and mask 0.png\n",
      "Skipping image 2.jpg and mask 2.png\n",
      "Skipping image 14.jpg and mask 14.png\n",
      "Skipping image 15.jpg and mask 15.png\n",
      "Skipping image 2052.jpg and mask 2052.png\n",
      "Skipping image 2053.jpg and mask 2053.png\n",
      "Skipping image 3048.jpg and mask 3048.png\n",
      "Skipping image 3059.jpg and mask 3059.png\n",
      "Skipping image 1061.jpg and mask 1061.png\n",
      "Skipping image 1079.jpg and mask 1079.png\n",
      "Min height: 219, Max height: 3648\n",
      "Min width: 330, Max width: 5472\n",
      "Total images: 280\n"
     ]
    }
   ],
   "source": [
    "def get_all_images_labels(IMAGES_DIR, LABELS_DIR, metadata):\n",
    "    \"\"\"\n",
    "    Return the list of all valid images and labels\n",
    "    \"\"\"\n",
    "    images = []\n",
    "    labels = []\n",
    "    min_h = 1e6; max_h = 0\n",
    "    min_w = 1e6; max_w = 0\n",
    "    for _, row in metadata.iterrows():\n",
    "        image_path = os.path.join(IMAGES_DIR, row['Image'])\n",
    "        label_path = os.path.join(LABELS_DIR, row['Mask'])\n",
    "        img_arr = np.array(Image.open(image_path))\n",
    "        label_arr = np.array(Image.open(label_path))\n",
    "        if (img_arr.ndim == 3) and (img_arr.shape[2] == 3) and (img_arr.shape[:-1] == label_arr.shape):\n",
    "            min_h = min(min_h, img_arr.shape[0]); max_h = max(max_h, img_arr.shape[0])\n",
    "            min_w = min(min_w, img_arr.shape[1]); max_w = max(max_w, img_arr.shape[1])\n",
    "            images.append(image_path)\n",
    "            labels.append(label_path)\n",
    "        else:\n",
    "            print(f\"Skipping image {row['Image']} and mask {row['Mask']}\")\n",
    "    print(f\"Min height: {min_h}, Max height: {max_h}\")\n",
    "    print(f\"Min width: {min_w}, Max width: {max_w}\")\n",
    "    return images, labels\n",
    "\n",
    "all_images, all_labels = get_all_images_labels(IMAGES_DIR, LABELS_DIR, metadata)\n",
    "print(f\"Total images: {len(all_images)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6af7583c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training images: 200\n",
      "Validation images: 20\n",
      "Test images: 60\n"
     ]
    }
   ],
   "source": [
    "# Split the data into training, validation, and test sets\n",
    "\n",
    "TRAIN_SIZE = 200\n",
    "VAL_SIZE = 20\n",
    "TEST_SIZE = 60\n",
    "\n",
    "image_paths_train = all_images[:TRAIN_SIZE]\n",
    "label_paths_train = all_labels[:TRAIN_SIZE]\n",
    "\n",
    "image_paths_val = all_images[TRAIN_SIZE:TRAIN_SIZE+VAL_SIZE]\n",
    "label_paths_val = all_labels[TRAIN_SIZE:TRAIN_SIZE+VAL_SIZE]\n",
    "\n",
    "image_paths_test = all_images[-TEST_SIZE:]\n",
    "label_paths_test = all_labels[-TEST_SIZE:]\n",
    "\n",
    "print(f\"Training images: {len(image_paths_train)}\")\n",
    "print(f\"Validation images: {len(image_paths_val)}\")\n",
    "print(f\"Test images: {len(image_paths_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5282f7d",
   "metadata": {},
   "source": [
    "# Dataset and Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "406c26f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FloodDataset(Dataset):\n",
    "    def __init__(self, image_paths, mask_paths, transform=None):\n",
    "        self.image_paths = image_paths\n",
    "        self.mask_paths = mask_paths\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image = np.array(Image.open(self.image_paths[idx]).convert(\"RGB\"))\n",
    "        mask = np.array(Image.open(self.mask_paths[idx]).convert(\"L\"))\n",
    "        \n",
    "        # Convert mask to binary\n",
    "        mask = (mask >= 128).astype('float32')\n",
    "        if self.transform:\n",
    "            augmented = self.transform(image=image, mask=mask)\n",
    "            image = augmented['image']\n",
    "            mask = augmented['mask']\n",
    "\n",
    "        # Move channels first\n",
    "        image = np.transpose(image, (2, 0, 1)).astype('float32')\n",
    "        return torch.tensor(image), torch.tensor(mask).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d1874b90",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing mean and std: 100%|██████████| 200/200 [00:18<00:00, 10.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean: [0.50019354 0.4970171  0.44732517], Std: [0.21560247 0.19379494 0.20825012]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Compute dataset mean and std for normalization\n",
    "def compute_mean_std(image_paths):\n",
    "    \"\"\"\n",
    "    Compute the mean and standard deviation of the dataset.\n",
    "    \"\"\"\n",
    "    means = []\n",
    "    stds = []\n",
    "    for path in tqdm(image_paths, desc=\"Computing mean and std\"):\n",
    "        image = np.array(Image.open(path).convert(\"RGB\")).astype('float32') / 255.0\n",
    "        means.append(np.mean(image, axis=(0,1)))\n",
    "        stds.append(np.std(image, axis=(0,1)))\n",
    "    mean = np.mean(means, axis=0)\n",
    "    std = np.mean(stds, axis=0)\n",
    "    return mean, std\n",
    "\n",
    "mean, std = compute_mean_std(image_paths_train)\n",
    "print(f\"Mean: {mean}, Std: {std}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4d3e2552",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define transformations\n",
    "H = 512\n",
    "W = 512\n",
    "\n",
    "# Convert mean and std to lists\n",
    "mean = mean.tolist()\n",
    "std = std.tolist()\n",
    "\n",
    "train_transform = A.Compose([\n",
    "    A.Resize(height=H, width=W),\n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    A.Normalize(mean=mean, std=std),\n",
    "])\n",
    "\n",
    "val_transform = A.Compose([\n",
    "    A.Resize(height=H, width=W),\n",
    "    A.Normalize(mean=mean, std=std),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b6eb83ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create datasets\n",
    "train_dataset = FloodDataset(image_paths_train, label_paths_train, transform=train_transform)\n",
    "val_dataset = FloodDataset(image_paths_val, label_paths_val, transform=val_transform)\n",
    "test_dataset = FloodDataset(image_paths_test, label_paths_test, transform=val_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "45b864af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data loaders\n",
    "BATCH_SIZE = 4\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fb52382",
   "metadata": {},
   "source": [
    "# Model Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "693cdff7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.5.1+cu124\n",
      "CUDA available: False\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "88fa8453",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: True\n",
      "CUDA Device: NVIDIA GeForce RTX 3060 Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Check if CUDA is available\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "\n",
    "# Print CUDA device\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA Device: {torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "    print(\"CUDA is not available.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "56fb2742",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded on cpu\n"
     ]
    }
   ],
   "source": [
    "# Initialize the UNet++ model\n",
    "model = smp.UnetPlusPlus(\n",
    "    encoder_name='resnet34',        # Choose encoder, e.g. resnet34 or efficientnet-b0\n",
    "    encoder_weights='imagenet',     # Use `imagenet` pre-trained weights for encoder initialization\n",
    "    in_channels=3,                  # Model input channels (1 for grayscale images, 3 for RGB)\n",
    "    classes=1,                      # Model output channels (number of classes)\n",
    "    activation=None,                # No activation, we'll apply Sigmoid during training\n",
    ")\n",
    "\n",
    "# Move model to device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "print(f'Model loaded on {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e79b2ded",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class DiceLoss(nn.Module):\n",
    "    def __init__(self, smooth=1.0):\n",
    "        super(DiceLoss, self).__init__()\n",
    "        self.smooth = smooth\n",
    "\n",
    "    def forward(self, outputs, targets):\n",
    "        # Apply sigmoid to get probabilities\n",
    "        outputs = torch.sigmoid(outputs)\n",
    "        # Flatten tensors\n",
    "        outputs = outputs.view(-1)\n",
    "        targets = targets.view(-1)\n",
    "        # Compute Dice Loss\n",
    "        intersection = (outputs * targets).sum()\n",
    "        dice_loss = 1 - (2. * intersection + self.smooth) / (outputs.sum() + targets.sum() + self.smooth)\n",
    "        return dice_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cc5f7c63",
   "metadata": {},
   "outputs": [],
   "source": [
    "def iou_metric(outputs, targets, threshold=0.5, smooth=1e-6):\n",
    "    outputs = torch.sigmoid(outputs)\n",
    "    outputs = (outputs > threshold).float()\n",
    "    targets = targets.float()\n",
    "    # Flatten tensors\n",
    "    outputs = outputs.view(-1)\n",
    "    targets = targets.view(-1)\n",
    "    # Compute IoU\n",
    "    intersection = (outputs * targets).sum()\n",
    "    total = (outputs + targets).sum()\n",
    "    union = total - intersection\n",
    "    iou = (intersection + smooth) / (union + smooth)\n",
    "    return iou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20f9cbbd",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'segmentation_models_pytorch' has no attribute 'utils'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Define loss function and metrics for SMP v0.3.4\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m loss_fn \u001b[38;5;241m=\u001b[39m \u001b[43msmp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mutils\u001b[49m\u001b[38;5;241m.\u001b[39mlosses\u001b[38;5;241m.\u001b[39mDiceLoss()\n\u001b[1;32m      3\u001b[0m metrics \u001b[38;5;241m=\u001b[39m [smp\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mmetrics\u001b[38;5;241m.\u001b[39mIoU(threshold\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m)]\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Define the optimizer\u001b[39;00m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'segmentation_models_pytorch' has no attribute 'utils'"
     ]
    }
   ],
   "source": [
    "# # Define loss function and metrics for SMP v0.3.4\n",
    "# loss_fn = smp.utils.losses.DiceLoss()\n",
    "# metrics = [smp.utils.metrics.IoU(threshold=0.5)]\n",
    "\n",
    "# # Define the optimizer\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c056d7c",
   "metadata": {},
   "source": [
    "# Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9fad52a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def train_one_epoch(model, loader, optimizer, loss_fn, device):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    total_iou = 0.0\n",
    "    for images, masks in tqdm(loader, desc=\"Training\", leave=False):\n",
    "        images = images.to(device)\n",
    "        masks = masks.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = loss_fn(outputs, masks)\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        # Calculate IoU\n",
    "        iou = iou_metric(outputs, masks)\n",
    "        total_iou += iou.item()\n",
    "    \n",
    "    avg_loss = total_loss / len(loader)\n",
    "    avg_iou = total_iou / len(loader)\n",
    "    return avg_loss, avg_iou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b694208c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model, loader, loss_fn, device):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    total_iou = 0.0\n",
    "    with torch.no_grad():\n",
    "        for images, masks in tqdm(loader, desc=\"Validation\", leave=False):\n",
    "            images = images.to(device)\n",
    "            masks = masks.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(images)\n",
    "            loss = loss_fn(outputs, masks)\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            # Calculate IoU\n",
    "            iou = iou_metric(outputs, masks)\n",
    "            total_iou += iou.item()\n",
    "    \n",
    "    avg_loss = total_loss / len(loader)\n",
    "    avg_iou = total_iou / len(loader)\n",
    "    return avg_loss, avg_iou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0d86f35d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define loss function\n",
    "loss_fn = DiceLoss()\n",
    "\n",
    "# Define optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a2cebd75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.3665, Train IoU: 0.6255\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 0.2329, Val IoU: 0.7984\n",
      "Model saved!\n",
      "\n",
      "Epoch 2/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Train\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m train_loss, train_iou \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_one_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrain Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Train IoU: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_iou\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Validate\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[15], line 17\u001b[0m, in \u001b[0;36mtrain_one_epoch\u001b[0;34m(model, loader, optimizer, loss_fn, device)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# Backward pass and optimization\u001b[39;00m\n\u001b[1;32m     16\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 17\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     20\u001b[0m total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[0;32m~/Documents/Deep Learning/dlProject/.venv/lib/python3.11/site-packages/torch/_tensor.py:581\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    572\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    573\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    574\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    579\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    580\u001b[0m     )\n\u001b[0;32m--> 581\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    582\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    583\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Deep Learning/dlProject/.venv/lib/python3.11/site-packages/torch/autograd/__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Deep Learning/dlProject/.venv/lib/python3.11/site-packages/torch/autograd/graph.py:825\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    823\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    824\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    826\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    827\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    828\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    829\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Number of epochs\n",
    "num_epochs = 10\n",
    "best_iou = 0.0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f'\\nEpoch {epoch+1}/{num_epochs}')\n",
    "    \n",
    "    # Train\n",
    "    train_loss, train_iou = train_one_epoch(model, train_loader, optimizer, loss_fn, device)\n",
    "    print(f\"Train Loss: {train_loss:.4f}, Train IoU: {train_iou:.4f}\")\n",
    "    \n",
    "    # Validate\n",
    "    val_loss, val_iou = validate(model, val_loader, loss_fn, device)\n",
    "    print(f\"Val Loss: {val_loss:.4f}, Val IoU: {val_iou:.4f}\")\n",
    "    \n",
    "    # Save the best model\n",
    "    if val_iou > best_iou:\n",
    "        best_iou = val_iou\n",
    "        torch.save(model.state_dict(), 'best_unetplusplus_model.pth')\n",
    "        print(\"Model saved!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d12cbf0",
   "metadata": {},
   "source": [
    "# Evaluation on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ae0ba64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the best saved model\n",
    "model.load_state_dict(torch.load('best_unetplusplus_model.pth'))\n",
    "model.eval()\n",
    "\n",
    "# Evaluation function\n",
    "def evaluate_model(model, loader, loss_fn, device):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    total_iou = 0.0\n",
    "    with torch.no_grad():\n",
    "        for images, masks in tqdm(loader, desc=\"Evaluating\"):\n",
    "            images = images.to(device)\n",
    "            masks = masks.to(device)\n",
    "            \n",
    "            outputs = model(images)\n",
    "            loss = loss_fn(outputs, masks)\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            iou = iou_metric(outputs, masks)\n",
    "            total_iou += iou.item()\n",
    "    \n",
    "    avg_loss = total_loss / len(loader)\n",
    "    avg_iou = total_iou / len(loader)\n",
    "    print(f\"Test Loss: {avg_loss:.4f}, Test IoU: {avg_iou:.4f}\")\n",
    "    return avg_loss, avg_iou\n",
    "\n",
    "# Evaluate on test set\n",
    "test_loss, test_iou = evaluate_model(model, test_loader, loss_fn, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38f1dccb",
   "metadata": {},
   "source": [
    "# Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7d5f7aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_predictions(model, dataset, num_samples=5):\n",
    "    model.eval()\n",
    "    for i in range(num_samples):\n",
    "        image, mask = dataset[i]\n",
    "        image_input = image.to(device).unsqueeze(0)\n",
    "        mask = mask.squeeze().cpu().numpy()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            output = model(image_input)\n",
    "            output = torch.sigmoid(output)\n",
    "            pred_mask = (output.squeeze().cpu().numpy() > 0.5).astype(np.uint8)\n",
    "        \n",
    "        # Denormalize the image for visualization\n",
    "        image_np = image.cpu().numpy()\n",
    "        image_np = np.transpose(image_np, (1, 2, 0))\n",
    "        image_np = (image_np * std + mean) * 255.0\n",
    "        image_np = np.clip(image_np, 0, 255).astype(np.uint8)\n",
    "        \n",
    "        plt.figure(figsize=(15, 5))\n",
    "        plt.subplot(1, 3, 1)\n",
    "        plt.imshow(image_np)\n",
    "        plt.title('Original Image')\n",
    "        plt.axis('off')\n",
    "        \n",
    "        plt.subplot(1, 3, 2)\n",
    "        plt.imshow(mask, cmap='gray')\n",
    "        plt.title('Ground Truth Mask')\n",
    "        plt.axis('off')\n",
    "        \n",
    "        plt.subplot(1, 3, 3)\n",
    "        plt.imshow(pred_mask, cmap='gray')\n",
    "        plt.title('Predicted Mask')\n",
    "        plt.axis('off')\n",
    "        \n",
    "        plt.show()\n",
    "\n",
    "# Visualize predictions on test set\n",
    "visualize_predictions(model, test_dataset, num_samples=5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
